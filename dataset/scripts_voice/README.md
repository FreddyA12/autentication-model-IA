# ğŸ¤ PIPELINE DE VOZ - README

## ğŸ“‹ Estructura del Pipeline

Este pipeline es **idÃ©ntico** al de reconocimiento facial, pero para voz:

```
ROSTROS:                           VOZ:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Videos                             Videos
  â†“                                  â†“
1_extract_frames.py                1_extract_audio.py
  â†“                                  â†“
Frames                             Audios WAV (16kHz)
  â†“                                  â†“
2_preprocess_embeddings.py         2_generate_voice_embeddings.py
  â†“                                  â†“
FaceNet (embeddings 512D)          ECAPA-TDNN (embeddings 192D)
  â†“                                  â†“
3_train_classifier.py              3_train_voice_mlp.py
  â†“                                  â†“
Tu CNN clasificador                Tu MLP clasificador
  â†“                                  â†“
4_predict.py                       4_predict_voice.py
```

## ğŸš€ Pasos para Entrenar

### Paso 0: Instalar Dependencias

```powershell
pip install -r requirements.txt
```

### Paso 1: Preparar Videos

Coloca los videos en `dataset/videos/`:

```
dataset/videos/
    freddy.mp4
    melanie.mp4
    rafael.mp4
    william.mp4
    ismael.mp4
```

**Requisitos:**
- Nombre del video = nombre de la persona
- Debe tener audio claro (2-5 segundos mÃ­nimo)

### Paso 2: Extraer Audio

```powershell
python dataset/scripts_voice/1_extract_audio.py
```

**Output:**
```
dataset/dataset_voice/
    freddy/
        freddy_001.wav (3s, 16kHz, mono)
        freddy_002.wav
        freddy_003.wav
    melanie/
        melanie_001.wav
    ...
```

### Paso 3: Generar Embeddings con ECAPA-TDNN

```powershell
python dataset/scripts_voice/2_generate_voice_embeddings.py
```

**Â¿QuÃ© hace?**
- Usa **ECAPA-TDNN preentrenado** (NO lo entrenas)
- Convierte cada audio â†’ vector de 192 nÃºmeros
- Guarda embeddings en `dataset/embeddings/`

**Output:**
```
dataset/embeddings/
    voice_embeddings.npy       # (N, 192)
    voice_labels.npy           # (N,)
    voice_label_map.json       # {0: 'freddy', 1: 'melanie', ...}
```

### Paso 4: Entrenar TU MLP

```powershell
python dataset/scripts_voice/3_train_voice_mlp.py
```

**Â¿QuÃ© hace?**
- Entrena **TU PROPIA** red neuronal
- Arquitectura: Dense(256) â†’ Dropout â†’ Dense(128) â†’ Dropout â†’ Softmax
- Usa epochs, backpropagation, etc.
- Guarda modelo en `dataset/models/voice_mlp_best.keras`

**Esto SÃ es entrenamiento supervisado con tu dataset.**

### Paso 5: Probar el Modelo

```powershell
python dataset/scripts_voice/4_predict_voice.py <audio.wav>
```

**Ejemplos:**
```powershell
# Probar con audio del dataset
python dataset/scripts_voice/4_predict_voice.py dataset/dataset_voice/freddy/freddy_001.wav

# Probar con audio nuevo
python dataset/scripts_voice/4_predict_voice.py test_audio.wav
```

## ğŸ§  Â¿QuÃ© se Entrena y QuÃ© No?

### âŒ NO entrenas ECAPA-TDNN
- Es un modelo preentrenado (como FaceNet)
- Ya sabe extraer caracterÃ­sticas de voz
- Solo lo usas para obtener embeddings

### âœ… SÃ entrenas el MLP
- Es **TU modelo**
- Lo entrenas desde cero con tus datos
- Aprende a clasificar los embeddings
- Tiene epochs, loss, accuracy, etc.

## ğŸ“Š Pipeline Completo

```
Audio â†’ ECAPA-TDNN â†’ Embedding(192) â†’ MLP â†’ Identidad
        (preentrenado)                (TU modelo)
```

## ğŸ”§ Archivos Clave

### Scripts
- `scripts_voice/1_extract_audio.py` - Extrae audio de videos
- `scripts_voice/2_generate_voice_embeddings.py` - ECAPA-TDNN embeddings
- `scripts_voice/3_train_voice_mlp.py` - Entrena tu MLP
- `scripts_voice/4_predict_voice.py` - Predice identidad

### Datos Generados
- `dataset/dataset_voice/` - Audios organizados por persona
- `dataset/embeddings/voice_*.npy` - Embeddings y labels
- `dataset/models/voice_mlp_best.keras` - Tu modelo entrenado

## ğŸ’¡ Tips

1. **MÃ­nimo de datos:**
   - 3-8 audios por persona
   - 2-5 segundos cada audio

2. **Calidad del audio:**
   - Sin ruido de fondo
   - Voz clara
   - 16 kHz (se hace automÃ¡ticamente)

3. **Troubleshooting:**
   - Si el modelo tiene baja accuracy â†’ mÃ¡s datos
   - Si hay overfitting â†’ mÃ¡s dropout
   - Si underfitting â†’ mÃ¡s epochs o neuronas

## ğŸ¯ Resultado Esperado

```
ğŸ“Š RESULTADOS DE PREDICCIÃ“N
================================================================

ğŸ¤ Audio: freddy_test.wav

ğŸ“ˆ Probabilidades por clase:
   freddy        95.32%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   melanie        3.21%  â–ˆâ–ˆ
   rafael         1.23%  â–ˆ
   william        0.18%  
   ismael         0.06%  

================================================================
âœ… IDENTIDAD: FREDDY
   Confianza: 95.32%
================================================================
```

## âš–ï¸ ComparaciÃ³n con Rostros

| CaracterÃ­stica | Rostros | Voz |
|---------------|---------|-----|
| **Extractor** | FaceNet (512D) | ECAPA-TDNN (192D) |
| **Clasificador** | CNN Dense | MLP Dense |
| **Input** | Frames 160x160 | Audio 16kHz |
| **Output** | Identidad + confianza | Identidad + confianza |
| **Â¿Se entrena extractor?** | âŒ No | âŒ No |
| **Â¿Se entrena clasificador?** | âœ… SÃ­ | âœ… SÃ­ |

---

**Â¡Listo!** Ahora tienes el pipeline de voz funcionando igual que el de rostros ğŸ‰
