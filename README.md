# Sistema de AutenticaciÃ³n BiomÃ©trica Dual con IA

Sistema de autenticaciÃ³n biomÃ©trica que combina **Reconocimiento Facial** y **Reconocimiento de Voz** para proporcionar una autenticaciÃ³n dual segura y precisa.

## ğŸ¯ CaracterÃ­sticas

- âœ… **AutenticaciÃ³n Dual**: Requiere coincidencia de rostro Y voz para autenticar
- âœ… **Reconocimiento Facial**: Usando MTCNN + FaceNet con 512-dim embeddings
- âœ… **Reconocimiento de Voz**: Usando ECAPA-TDNN con 192-dim embeddings (estado del arte)
- âœ… **Interfaz Web**: Django backend con interfaz moderna en HTML/CSS/JS
- âœ… **DetecciÃ³n de Desconocidos**: Rechaza automÃ¡ticamente personas no registradas
- âœ… **Alta PrecisiÃ³n**: 100% accuracy en tests con ECAPA-TDNN

---

## ğŸ“‹ Requisitos Previos

- Python 3.8+
- Webcam
- MicrÃ³fono
- Windows/Linux/macOS

---

## ğŸš€ InstalaciÃ³n

### 1. Crear y Activar Entorno Virtual

**Windows:**
```powershell
python -m venv venv
.\venv\Scripts\activate
```

**Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

**Nota importante**: El proyecto usa versiones especÃ­ficas de PyTorch (2.6.0) por compatibilidad con SpeechBrain.

---

## ğŸ§  Arquitectura del Sistema

### Reconocimiento Facial

#### TecnologÃ­as
- **MTCNN**: Detector de rostros multi-tarea
- **FaceNet (Keras)**: Extractor de embeddings (512 dimensiones)
- **Clasificador MLP**: Red neuronal para clasificaciÃ³n de identidades

#### Pipeline de Entrenamiento

**UbicaciÃ³n de scripts:** `dataset/face/scripts/`

##### 1. Extraer Frames de Videos
Extrae fotogramas de videos de entrenamiento.
```bash
python dataset/face/scripts/1_extract_frames.py
```
- **Input**: Videos en `dataset/face/videos/`
- **Output**: Frames en `dataset/face/processed/`
- **ParÃ¡metros**: 1 frame cada 5 frames del video

##### 2. Preprocesar y Extraer Embeddings
Detecta rostros, los alinea y extrae embeddings faciales.
```bash
python dataset/face/scripts/2_preprocess_and_extract_embeddings.py
```
- **Proceso**:
  1. Detecta rostros usando MTCNN
  2. Recorta y redimensiona a 160x160 px
  3. Extrae embedding de 512 dimensiones con FaceNet
- **Output**: `dataset/face/embeddings/embeddings_dataset.pkl`

##### 3. Entrenar Clasificador
Entrena un MLP para clasificar los embeddings faciales.
```bash
python dataset/face/scripts/3_train_classifier.py
```
- **Arquitectura del MLP**:
  - Input: 512 dimensiones
  - Dense(256) + Dropout(0.3)
  - Dense(128) + Dropout(0.2)
  - Output: Softmax (nÃºmero de personas)
- **Output**: `dataset/face/models/face_classifier.keras`

##### 4. Probar el Modelo
EvalÃºa el modelo con imÃ¡genes de prueba.
```bash
python dataset/face/scripts/4_predict.py
```
- **Input**: ImÃ¡genes en `dataset/face/test_data/`
- **Output**: Predicciones con confianza

---

### Reconocimiento de Voz

#### TecnologÃ­as
- **ECAPA-TDNN**: Modelo de speaker recognition (SpeechBrain)
- **MLP Classifier**: Red neuronal para clasificaciÃ³n de voces
- **Embeddings**: 192 dimensiones optimizadas para distinguir voces

#### Pipeline de Entrenamiento

**UbicaciÃ³n de scripts:** `dataset/voice/scripts/`

##### 1. Extraer y Procesar Audio
Limpia audio, elimina silencios y segmenta en clips.
```bash
python dataset/voice/scripts/1_extract_audio.py
```
- **Input**: Archivos de audio en `dataset/voice/audio_raw/`
- **Proceso**:
  1. Resamplea a 16kHz mono
  2. Elimina silencios (VAD)
  3. Segmenta en clips de 5 segundos
- **Output**: Archivos WAV en `dataset/voice/processed/`

##### 2. Generar Embeddings con ECAPA-TDNN
Extrae embeddings de voz usando el modelo ECAPA-TDNN pre-entrenado.
```bash
python dataset/voice/scripts/2_generate_voice_embeddings.py
```
- **Modelo**: `speechbrain/spkrec-ecapa-voxceleb`
- **Proceso**:
  1. Carga audio (mono 16kHz)
  2. Extrae embedding de 192 dimensiones
  3. Cada audio genera un vector Ãºnico
- **Output**: 
  - `dataset/voice/embeddings/voice_embeddings.npy`
  - `dataset/voice/embeddings/voice_labels.npy`
  - `dataset/voice/embeddings/voice_label_map.json`

**Â¿Por quÃ© ECAPA-TDNN y no YAMNet?**
- **ECAPA-TDNN**: DiseÃ±ado especÃ­ficamente para speaker recognition (192 dims)
- **YAMNet**: Clasificador genÃ©rico de sonidos (1024 dims) - menos preciso para voces

##### 3. Entrenar Clasificador MLP
Entrena un MLP para clasificar los embeddings de voz.
```bash
python dataset/voice/scripts/3_train_voice_mlp.py
```
- **Arquitectura del MLP**:
  - Input: 192 dimensiones
  - Dense(512) + BatchNorm + Dropout(0.3)
  - Dense(256) + BatchNorm + Dropout(0.4)
  - Output: Softmax (nÃºmero de personas + unknown)
- **Entrenamiento**:
  - Optimizer: Adam (lr=0.001)
  - Loss: Sparse Categorical Crossentropy
  - Callbacks: EarlyStopping, ReduceLROnPlateau
  - Data Augmentation: Ruido gaussiano si dataset < 100 muestras
- **Output**: `dataset/voice/models/voice_mlp_best.keras`

##### 4. Evaluar el Modelo
Prueba el modelo con audios de test.
```bash
python dataset/voice/scripts/4_predict_voice.py
```
- **Input**: Archivos de audio en `dataset/voice/test_audios/`
- **Output**: Predicciones con confianza
- **Sanity Check**: Verifica precisiÃ³n en el dataset de entrenamiento

---

## ğŸŒ AplicaciÃ³n Web (Django)

### Iniciar el Servidor

```bash
cd web
python manage.py runserver
```

La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:8000`

### Endpoints API

#### 1. Reconocimiento Facial
```
POST /api/predict/
Content-Type: multipart/form-data
Body: { image: <archivo> }
```

#### 2. Reconocimiento de Voz
```
POST /api/predict_voice/
Content-Type: multipart/form-data
Body: { audio: <archivo WAV> }
```

#### 3. AutenticaciÃ³n Dual
```
POST /api/authenticate_dual/
Content-Type: multipart/form-data
Body: { 
  image: <archivo>,
  audio: <archivo WAV>
}
```

### LÃ³gica de AutenticaciÃ³n Dual

El sistema implementa una autenticaciÃ³n dual con las siguientes reglas:

1. âœ… **Ambos coinciden**: AutenticaciÃ³n exitosa
2. âœ… **Solo cara exitosa (>90%)**: Permite acceso (voz opcional)
3. âŒ **Solo voz exitosa**: Rechaza acceso (requiere cara)
4. âŒ **Ninguno exitoso**: Rechaza acceso

**ConfiguraciÃ³n de umbrales** (`web/settings.py`):
- `CONFIDENCE_THRESHOLD = 0.95` (95% para cara)
- `VOICE_CONFIDENCE_THRESHOLD = 0.85` (85% para voz)

---

## ğŸ“ Estructura del Proyecto

```
autentication-model-IA/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ face/                    # Reconocimiento facial
â”‚   â”‚   â”œâ”€â”€ videos/              # Videos de entrenamiento
â”‚   â”‚   â”œâ”€â”€ processed/           # Frames extraÃ­dos
â”‚   â”‚   â”œâ”€â”€ aligned/             # Rostros alineados
â”‚   â”‚   â”œâ”€â”€ embeddings/          # Embeddings faciales
â”‚   â”‚   â”œâ”€â”€ models/              # Modelos entrenados
â”‚   â”‚   â”‚   â”œâ”€â”€ face_classifier.keras
â”‚   â”‚   â”‚   â””â”€â”€ class_indices.json
â”‚   â”‚   â””â”€â”€ scripts/             # Scripts de entrenamiento
â”‚   â”‚
â”‚   â””â”€â”€ voice/                   # Reconocimiento de voz
â”‚       â”œâ”€â”€ audio_raw/           # Audio crudo
â”‚       â”œâ”€â”€ processed/           # Audio procesado
â”‚       â”œâ”€â”€ embeddings/          # Embeddings de voz
â”‚       â”œâ”€â”€ models/              # Modelos entrenados
â”‚       â”‚   â”œâ”€â”€ voice_mlp_best.keras
â”‚       â”‚   â””â”€â”€ voice_class_indices.json
â”‚       â”œâ”€â”€ test_audios/         # Audios de prueba
â”‚       â””â”€â”€ scripts/             # Scripts de entrenamiento
â”‚
â”œâ”€â”€ web/                         # AplicaciÃ³n Django
â”‚   â”œâ”€â”€ face_auth/               # App principal
â”‚   â”‚   â”œâ”€â”€ face_service.py      # Servicio de reconocimiento facial
â”‚   â”‚   â”œâ”€â”€ voice_service.py     # Servicio de reconocimiento de voz
â”‚   â”‚   â”œâ”€â”€ views.py             # Endpoints API
â”‚   â”‚   â”œâ”€â”€ templates/           # HTML
â”‚   â”‚   â””â”€â”€ static/              # CSS/JS
â”‚   â”œâ”€â”€ settings.py              # ConfiguraciÃ³n Django
â”‚   â””â”€â”€ manage.py                # CLI Django
â”‚
â”œâ”€â”€ pretrained_models/           # Modelos pre-entrenados descargados
â”‚   â””â”€â”€ spkrec-ecapa-voxceleb/   # ECAPA-TDNN de SpeechBrain
â”‚
â”œâ”€â”€ requirements.txt             # Dependencias
â””â”€â”€ README.md                    # Este archivo
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar Umbrales de Confianza

Edita `web/settings.py`:

```python
# Umbral para reconocimiento facial (0.0 - 1.0)
CONFIDENCE_THRESHOLD = 0.95  

# Umbral para reconocimiento de voz (0.0 - 1.0)
VOICE_CONFIDENCE_THRESHOLD = 0.85
```

**Recomendaciones**:
- **Cara**: 0.90 - 0.95 (muy preciso)
- **Voz**: 0.70 - 0.85 (balance entre precisiÃ³n y usabilidad)

### Agregar Nuevas Personas

#### Reconocimiento Facial
1. Graba un video corto (10-30 segundos) de la persona
2. GuÃ¡rdalo en `dataset/face/videos/<nombre>/`
3. Ejecuta el pipeline completo desde el paso 1

#### Reconocimiento de Voz
1. Graba 3-5 audios de la persona hablando (5-10 segundos cada uno)
2. GuÃ¡rdalos en `dataset/voice/audio_raw/<nombre>/`
3. Ejecuta el pipeline completo desde el paso 1

---

## ğŸ“Š Rendimiento del Sistema

### Reconocimiento Facial
- **PrecisiÃ³n**: ~98-100% en personas registradas
- **FPS**: ~2-3 fps en CPU
- **Embeddings**: 512 dimensiones (FaceNet)

### Reconocimiento de Voz
- **PrecisiÃ³n**: 100% en tests con ECAPA-TDNN
- **Tiempo de inferencia**: ~1-2 segundos por audio
- **Embeddings**: 192 dimensiones (ECAPA-TDNN)
- **Mejora vs YAMNet**: +50% en precisiÃ³n para speaker recognition

---

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Error: "No se encontrÃ³ el modelo"
AsegÃºrate de haber ejecutado los scripts de entrenamiento completos:
```bash
# Para cara
python dataset/face/scripts/3_train_classifier.py

# Para voz
python dataset/voice/scripts/3_train_voice_mlp.py
```

### Error: "No se detectÃ³ ningÃºn rostro"
- Verifica que hay buena iluminaciÃ³n
- AsegÃºrate de estar mirando directamente a la cÃ¡mara
- Ajusta la distancia a la cÃ¡mara (30-100 cm recomendado)

### Error: "Voz no reconocida"
- Habla claramente durante 3-5 segundos
- Evita ruido de fondo excesivo
- Verifica que el micrÃ³fono funciona correctamente

### Error de compatibilidad con PyTorch/SpeechBrain
Reinstala las versiones especÃ­ficas:
```bash
pip install torch==2.6.0 torchaudio==2.6.0 --force-reinstall
```

---

## ğŸ“ Notas TÃ©cnicas

### Embeddings vs ClasificaciÃ³n Directa

El sistema usa un enfoque de **dos etapas**:

1. **ExtracciÃ³n de embeddings**: Modelos pre-entrenados (FaceNet, ECAPA-TDNN)
2. **ClasificaciÃ³n**: MLP entrenado con tus datos

**Ventajas**:
- Reutiliza conocimiento de modelos pre-entrenados
- Requiere menos datos de entrenamiento
- Mejor generalizaciÃ³n
- FÃ¡cil agregar nuevas personas (solo reentrenar el MLP)

### Por quÃ© ECAPA-TDNN

**ECAPA-TDNN** (Emphasized Channel Attention, Propagation and Aggregation in Time Delay Neural Network):
- Estado del arte en speaker recognition
- Embeddings de 192 dims optimizados para voces
- Pre-entrenado en VoxCeleb (millones de voces)
- Robusto a variaciones de micrÃ³fono y ruido

---

## ğŸ“š Referencias

- **FaceNet**: [Schroff et al., 2015](https://arxiv.org/abs/1503.03832)
- **ECAPA-TDNN**: [Desplanques et al., 2020](https://arxiv.org/abs/2005.07143)
- **SpeechBrain**: [Ravanelli et al., 2021](https://arxiv.org/abs/2106.04624)
- **MTCNN**: [Zhang et al., 2016](https://arxiv.org/abs/1604.02878)

---

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado como proyecto de autenticaciÃ³n biomÃ©trica con IA.

## ğŸ“„ Licencia

MIT License - Ver archivo LICENSE para mÃ¡s detalles.
